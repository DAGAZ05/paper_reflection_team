针对后续工作的建议与意见
1. 统一接口标准 (API Contract)
所有审计组必须提供一个标准的 FastAPI 接口，供中枢组和反思评估组调用。
请求接口： POST /audit
请求体 (JSON)：
JSON
{
  "paper_id": "uuid-string",
  "callback_url": "http://...", // 异步回调（可选）
  "audit_scope": ["abstract", "methodology", "code"], // 指定需要审计的章节
  "model_preference": "gpt-4o" // 可选
}
返回体 (JSON) - 必须包含以下核心字段： 各组内部逻辑可以不同，但吐出的 JSON 必须长得一样，方便反思组合并：
JSON
{
  "group_id": 6, 
  "audit_results": [
    {
      "id": "item-001",
      "point": "统计学显著性检验", // 审核点
      "score": 85,               // 评分 (0-100)
      "level": "Warning",        // 级别: Critical/Warning/Info
      "description": "实验三数据分布不均，未进行正态性检验。", 
      "evidence_quote": "原文第4.2节提到：'我们直接采用了T检验...'", // 必须从原文摘录
      "location": {"section": "4.2", "line_start": 45}, // 方便前端跳转
      "suggestion": "建议补充Shapiro-Wilk检验。"
    }
  ]
}

2. 各组具体任务指令
A. 中枢组 (中轴线)：
任务A： 实现：PDF -> MinerU解析 -> Markdown切片入库 -> 触发审计组接口 的全自动流程。
任务B： 一个简单的 Web 后台页面，点击“开始评审”，能看到各组接口的调用状态（Loading -> Success）。
任务C：统一数据库视图。解决文献组提到的 essay_ 前缀与架构指南中 paper_ 前缀不一致的问题，下发最终版表结构文档。
任务D：建立 API 调度中心。不再使用 Mock 数据，而是根据 paper_id 真正去异步触发各审计组的 /audit 接口。

B. 审计组 (格式、代码、实验、逻辑、文献)：
任务： 主动联系中枢组，确认你们的 DB_URL 是一致的。
交付物： 确保你们的 Agent 能够根据中枢组给的 paper_id，去 paper_sections 表里精准拉取自己负责的内容，而不是读本地 txt 文件。
（1）格式审计组
核心任务：完善动态规则库与布局校验逻辑
任务 A： 完善 rules.yaml。将周报中提到的“标题层级”、“图表位置”、“公式对齐”等规则量化，确保 AI 能根据这些硬性指标打分。目前各组（如格式组、实验组）都在自己写 rules.yaml 或搜索统计标准，但这部分数据应该统一存入中枢组的 expert_comments 表。 别把规则写死在代码里，要引导他们把规则变成“向量数据”存进数据库。参考之前发的规则。
任务 B： 实现 锚点生成。在返回的 JSON 中，必须包含错误位置在原文中的特征字符（用于前端高亮定位）。
任务 C： 配合中枢组测试 长文档处理。确保在处理几十页的论文时，异步任务不会因为超时而挂掉。

（2）代码审计组
核心任务：深化 RIG（依赖关系图）与代码一致性检查
任务 A： 实现 代码与描述的匹配性检查。利用 Agent 对比论文中的“伪代码/算法描述”与附件中的“实际源码”，检查是否存在挂羊头卖狗肉的情况。
任务 B： 完成 README 评分器集成。将其作为一个子模块集成到 /audit 接口中，提供对附件完整性的量化分值。
任务 C： 联调 Pydantic 输出。确保返回的 error_line_number 能够准确对应到附件文件的具体行。

（3）实验数据组
核心任务：构建“统计学专家”知识库
任务 A： 知识库入库。将搜集的统计学标准（如 P值检验要求、样本量要求）转化为向量数据，存入 expert_comments 表。
任务 B： 优化 System Prompt。让 Agent 具备识别“图表数据异常”的能力（例如：正文说准确率 95%，但图表曲线只到 80%）。
任务 C： 测试对 复杂表格 的读取能力（配合中枢组提供的 Markdown 表格数据）。

（4）逻辑审计组
核心任务：利用 NLI（自然语言推理）检查论证严密性
任务 A： 实现 跨章节逻辑校验。例如：检查“摘要中的结论”是否在“实验章节”中有数据支撑
任务 B： 定义 JSON Schema。确保逻辑矛盾点能以结构化的方式返回，包括矛盾的两个端点位置。

（5）文献真实性组
核心任务：实现参考文献的“真伪核验”
任务 A： 开发文献提取脚本。自动从论文的 Reference 章节提取篇名、作者和年份。
任务 B： 接入外部校验 API（如 Crossref 或 Google Scholar 模拟检索），核实文献是否存在，并标记“疑似虚假文献”。
任务 C： 规范化数据库。将 essay_ 前缀表名与中枢组对齐。

C. 反思评估组 (总收官)：
任务 A： 编写 冲突裁决逻辑。当不同审计组返回的 JSON 意见不一致时，调用“主考官 Agent”进行加权投票。当两个组意见冲突（如格式组说表格好，实验组说数据乱）时，你们的 Agent 是如何“裁决”并给出最终结论的。
任务 B： 实现 幻觉过滤检测。编写校验程序，核对各组返回的 evidence_quote 是否真的在数据库原文中能检索到。
任务 C： 最终 Markdown 报告模版。设计一套符合导师审阅习惯的评审报告模版，将各组结果拼装成一篇通顺的文章。

3. 操作清单

（1）由中枢组组长周煊博建立“联调文档”： 让各组长在腾讯共享文档里填上自己的 API Endpoint（例如：http://192.168.1.10:8001/audit）。
（2）强制“证据关联”： 严禁 AI 瞎编建议。要求所有组返回的评语必须带上 evidence_quote（原文证据）。没有证据的差评一律视为“幻觉”，由反思组直接剔除。