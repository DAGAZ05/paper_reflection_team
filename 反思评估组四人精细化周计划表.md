# 反思评估组（组7）四人精细化周计划表

## ✅ 严格分组：2人幻觉评估组｜2人对话/交互开发组｜每周个人任务+团队交付物

---

## 📌 分组与核心职责
| 组别                | 成员      | 核心任务                           | 技术栈聚焦                                         |
| ------------------- | --------- | ---------------------------------- | -------------------------------------------------- |
| **幻觉评估组**      | **成员A** | 冲突裁决、整体评分、项目进度调控   | LiteLLM（加权投票）、NumPy、规则引擎、Pydantic     |
|                     | **成员B** | 重复过滤、幻觉过滤                 | Sentence-Transformers、asyncpg、DBSCAN、余弦相似度 |
| **对话/交互开发组** | **成员C** | 导师对话生成                       | LiteLLM、Prompt模板库、正则语气优化器、JSON Schema |
|                     | **成员D** | 优先级排序、人工复核标记、系统集成 | 规则引擎、FastAPI、httpx、LangGraph                |

---

## 📅 四周个人任务计划（按组清晰划分）

### 🔍 **幻觉评估组（成员A + 成员B）**

| 周次      | 成员A（冲突裁决/整体评分）                                   | 成员B（重复过滤/幻觉过滤）                                   |
| --------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **第1周** | • 精读LLM-as-a-Judge核心论文（Judging LLM-as-a-Judge等）• 设计冲突裁决规则库（关键词矛盾库+加权投票逻辑）• 输出《冲突裁决算法设计草案》 | • 调研幻觉检测技术方案（精确匹配 vs 语义匹配）• 设计重复过滤聚类方案（相似度阈值设定）• 输出《证据验证技术选型报告》 |
| **第2周** | • 实现冲突裁决核心模块（处理Agent矛盾结论）• 开发整体评分计算引擎（加权平均+矛盾惩罚）• 编写单元测试（覆盖5类矛盾场景） | • 实现重复过滤模块（Sentence-Transformers聚类）• 开发幻觉过滤模块（数据库精确+语义双验证）• 编写验证测试用例（10处引用真实性检查） |
| **第3周** | • 优化裁决逻辑（处理边界矛盾案例）• 与对话组联调：输出结构化矛盾数据• 添加性能监控埋点 | • 优化数据库查询（添加section索引提速40%）• 与对话组联调：传递验证结果与置信度• 修复验证漏报问题 |
| **第4周** | • 编写《冲突裁决模块技术文档》• 参与10篇论文全量验证• 沉淀算法调优经验库 | • 整理10篇测试论文验证数据集• 编写《证据验证模块使用指南》• 修复遗留边界问题 |

---

### 💬 **对话/交互开发组（成员C + 成员D）**

| 周次      | 成员C（导师对话生成）                                        | 成员D（优先级排序/复核标记/集成）                            |
| --------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **第1周** | • 调研学术导师对话范式（收集50+真实对话样本）• 设计三类对话模板框架（质疑型/引导型/澄清型）• 输出《对话生成技术方案V1》 | • 设计复核标记规则（置信度阈值+冲突Agent数）• 制定优先级排序权重表（Critical/Major/Minor）• 输出《复核决策逻辑设计稿》 |
| **第2周** | • 实现对话生成引擎（LiteLLM调用+模板选择器）• 开发语气优化器（替换生硬表述）• 生成3类场景对话样本供评审 | • 实现优先级排序模块（规则引擎驱动）• 开发人工复核标记逻辑（动态阈值）• 编写复核标记单元测试 |
| **第3周** | • 优化Prompt注入领域知识（自动识别论文领域）• 与幻觉组联调：接收复核依据生成对话• 开发对话质量自动评分工具 | • 完成全链路集成（串联所有模块）• 对接Orchestrator（任务触发/结果回调）• 优化缓存机制提升生成效率 |
| **第4周** | • 编写《用户手册：导师对话解读指南》• 扩充典型场景对话示例库（10+案例）• 参与人工对话质量评审 | • 编写《系统集成与部署文档》• 主导人工复核标记准确率验证• 制作效果对比展示材料（视频+截图） |

---

## 🤝 每周团队交付物（老师会议专用）

| 周次      | 交付物名称                     | 核心内容                                                     | 发言要点                                                     |
| --------- | ------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **第1周** | 《技术调研与方案设计报告V1.0》 | • LLM-as-a-Judge论文精读摘要（3篇）• 冲突裁决/幻觉过滤/对话生成技术选型对比• 两组接口规范（JSON Schema草案）• 详细任务分工表 | “本周完成技术深度调研，确定**规则引擎为主+LLM辅助**的混合方案，两组接口规范已对齐，为开发奠定基础。” |
| **第2周** | 《核心模块原型验证报告》       | • 幻觉组：5篇论文矛盾检测准确率85%• 对话组：3类对话样本人工评分4.2/5.0• 重复过滤聚类效果截图• 复核标记逻辑测试用例 | “各模块原型验证通过：重复过滤合并相似建议准确率85%，导师对话获初步好评。下一步重点优化阈值与Prompt。” |
| **第3周** | 《全链路集成测试报告》         | • 端到端流程贯通截图（Agent结果→带对话报告）• 性能数据：P95响应时间2.8s，错误率0.5%• 优化亮点：DB查询提速40%、对话多样性提升• 10篇测试论文对比数据 | “系统实现完整闭环，关键指标达标。重点优化了数据库效率与对话质量，复核标记准确率提升至88%，接近目标。” |
| **第4周** | 《最终交付包+项目复盘》        | • 可运行Docker镜像+部署文档• 验证报告：复核标记准确率92%，对话专业度4.6/5.0• 知识资产：Prompt模板库、问题处理手册• 3分钟演示视频（全流程展示） | “项目超额完成：复核标记准确率92%，对话获专家认可。沉淀可复用模板与规则，为后续迭代提供坚实基础。感谢老师指导！” |

---

## 💡 关键协作机制
- **每周一**：两组同步接口进展（成员A↔成员D确认数据格式）
- **每周三**：交叉验证（成员B向成员C提供验证结果，成员D向成员A反馈复核需求）
- **每周五**：联合测试（使用统一测试论文集验证全链路）
- **交付物共建**：每份报告由两组共同撰写（幻觉组负责技术细节，对话组负责交互效果描述）

> ✅ **方案优势**：  
> 1. **责任绝对清晰**：每项任务归属唯一责任人，无模糊地带  
> 2. **组内深度聚焦**：幻觉组专注“技术严谨性”，对话组专注“人文温度”  
> 3. **交付物直击重点**：每周交付物紧扣老师关注点（调研深度→原型效果→集成质量→最终价值）  
> 4. **协作无缝衔接**：通过标准化接口与固定协作节点，确保两组高效协同  

本计划确保4人团队在4周内高质量交付，每周会议均有扎实成果汇报，完美支撑项目进度管理与成果展示。